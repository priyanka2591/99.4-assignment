{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trial3_8106_99.17t_99.34v.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ky3f_Odl-7um"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanka2591/99.4-assignment/blob/main/trial3_8106_99_17t_99_34v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky3f_Odl-7um"
      },
      "source": [
        "## Data Transformations\n",
        "\n",
        "We first start with defining our data transformations. We need to think what our data is and how can we augment it to correct represent images which it might not see otherwise. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNdGZiNDJxpY"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                   transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                   ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                   ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQciFYo2B1mO"
      },
      "source": [
        "# Dataset and Creating Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4A84rlfDA23"
      },
      "source": [
        "train = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)\n",
        "test = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgldp_3-Dn0c"
      },
      "source": [
        "# Dataloader Arguments & Test/Train Dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8OLDR79DrHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db08ed8d-73d6-4b22-e2dc-92ca0e5ab6dd"
      },
      "source": [
        "SEED = 1\n",
        "\n",
        "# CUDA?\n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"CUDA Available?\", cuda)\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if cuda:\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# dataloader arguments - something you'll fetch these from cmdprmt\n",
        "dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n",
        "\n",
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train, **dataloader_args)\n",
        "\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(test, **dataloader_args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Available? True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQL3H6RJL3h"
      },
      "source": [
        "# The model\n",
        "Let's start with the model we first saw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1NIcqbqVxjh"
      },
      "source": [
        "##### Train and Test functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSQdSBSrvx_p"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    \n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "    train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk2YKUY5bLCO"
      },
      "source": [
        "#### Trial 3 Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOiWzincczDr"
      },
      "source": [
        "Trial 3 <br>\n",
        "Target: <br>\n",
        "1) The target here is to reduce the number of parameters further by adding a max pooling in the second convoluitonal block after the 1x1 block.<br>\n",
        "2) The dropout layers are added only in the last layer of the convolutional block. <br>\n",
        "3) This has been done as the number of parameters have been reduced, retaining information is important which is done by reducing nunmber of drop out layers.   \n",
        "\n",
        "Results:<br>\n",
        "1) Model has been built with \n",
        "\n",
        "2 convolution blocks -> + Dropout last layer \n",
        "1 transition layer  -> \n",
        "2 conv block->  + Dropout last layer\n",
        "1 transition layer ->\n",
        "2 conv blocks -> + Dropout both layers\n",
        "gap layer - >\n",
        "final output\n",
        "\n",
        "2)Number of parameters- 8176 <br>\n",
        "  Ideal Training accuracy at epoch 13 is 99.08 <br> \n",
        "  Ideal Validation accurayc at epoch 13 is  99.40  <br>\n",
        "  5 epochs (E10-E14) validation accuracy 99.08-99.35 -> 99.13-99.36 -> 99.12-99.38 -> 99.08-99.35 -> 99.17-99.34\n",
        "\n",
        "Analysis:<br>\n",
        "1) Model has been built with batchnormalization at every layer.<br>\n",
        "2) After every convolution block (two convolutional layers) a transition layer has been introduced in order to reduce the number of channels and also to retain as much information as possible. <br>\n",
        " 3) Droputs have been reduced here thereby increasing the training accuracy of  the model than the previous models for the same epochs<br>\n",
        "4) The initial validation accuracy starts with 97.75\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yESu5Uby9vB8"
      },
      "source": [
        "#### Trial 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOzqjn5h3724"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "dropout_value = 0.05\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "#####################################################################################################\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 3), padding=0, bias=False), \n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(8),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        ) # input size = 28 output_size = 26, \n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # input_size = 26 , output_size = 24, 16 to 32 channels\n",
        "#####################################################################################################\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # input size  = 24, output_size = 24\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # input sizez = 24, output_size = 12\n",
        "#####################################################################################################\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=12, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(12),\n",
        "            # nn.Dropout(dropout_value)\n",
        "        ) # input size = 12, output_size = 10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=12, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # input size = 10, output_size = 8\n",
        "#####################################################################################################\n",
        "\n",
        "        # adding a 1x1 kernel  block here to reduce paramters\n",
        "        self.convblock5a = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) # input size  = 8, output_size = 8\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # input sizez = 8, output_size = 4\n",
        "#####################################################################################################\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=10, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # input size  = 4,  output_size =4 \n",
        "        self.convblock7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.ReLU(),            \n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(dropout_value)\n",
        "        ) # input size  = 4, output_size = 4\n",
        "#####################################################################################################        \n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=4)\n",
        "        ) # output_size = 1 channels  = 16\n",
        "\n",
        "        self.convblock8 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),\n",
        "        ) \n",
        "        self.dropout = nn.Dropout(dropout_value)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.convblock5a(x)\n",
        "        x = self.pool1(x)        \n",
        "        x = self.convblock6(x)\n",
        "        x = self.convblock7(x)\n",
        "        x = self.gap(x)        \n",
        "        x = self.convblock8(x)\n",
        "\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X9OrXZF5x-Q",
        "outputId": "1a525887-ba94-44f3-8979-8c49c802bce9"
      },
      "source": [
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 26, 26]              72\n",
            "              ReLU-2            [-1, 8, 26, 26]               0\n",
            "       BatchNorm2d-3            [-1, 8, 26, 26]              16\n",
            "            Conv2d-4           [-1, 16, 24, 24]           1,152\n",
            "              ReLU-5           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-6           [-1, 16, 24, 24]              32\n",
            "           Dropout-7           [-1, 16, 24, 24]               0\n",
            "            Conv2d-8            [-1, 8, 24, 24]             128\n",
            "         MaxPool2d-9            [-1, 8, 12, 12]               0\n",
            "           Conv2d-10           [-1, 12, 10, 10]             864\n",
            "             ReLU-11           [-1, 12, 10, 10]               0\n",
            "      BatchNorm2d-12           [-1, 12, 10, 10]              24\n",
            "           Conv2d-13             [-1, 16, 8, 8]           1,728\n",
            "             ReLU-14             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-15             [-1, 16, 8, 8]              32\n",
            "          Dropout-16             [-1, 16, 8, 8]               0\n",
            "           Conv2d-17             [-1, 10, 8, 8]             160\n",
            "        MaxPool2d-18             [-1, 10, 4, 4]               0\n",
            "           Conv2d-19             [-1, 16, 4, 4]           1,440\n",
            "             ReLU-20             [-1, 16, 4, 4]               0\n",
            "      BatchNorm2d-21             [-1, 16, 4, 4]              32\n",
            "          Dropout-22             [-1, 16, 4, 4]               0\n",
            "           Conv2d-23             [-1, 16, 4, 4]           2,304\n",
            "             ReLU-24             [-1, 16, 4, 4]               0\n",
            "      BatchNorm2d-25             [-1, 16, 4, 4]              32\n",
            "          Dropout-26             [-1, 16, 4, 4]               0\n",
            "        AvgPool2d-27             [-1, 16, 1, 1]               0\n",
            "           Conv2d-28             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 8,176\n",
            "Trainable params: 8,176\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.53\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.56\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JpBJaRv5yBs",
        "outputId": "67e23bef-f655-4040-a572-1348cd9dcae4"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "model =  Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    scheduler.step()\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Loss=0.18379493057727814 Batch_id=468 Accuracy=91.23: 100%|██████████| 469/469 [00:14<00:00, 33.38it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0808, Accuracy: 9775/10000 (97.75%)\n",
            "\n",
            "EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0981626808643341 Batch_id=468 Accuracy=97.79: 100%|██████████| 469/469 [00:14<00:00, 33.20it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0479, Accuracy: 9856/10000 (98.56%)\n",
            "\n",
            "EPOCH: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04015262424945831 Batch_id=468 Accuracy=98.20: 100%|██████████| 469/469 [00:13<00:00, 33.57it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "EPOCH: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.0251707062125206 Batch_id=468 Accuracy=98.41: 100%|██████████| 469/469 [00:14<00:00, 33.06it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9906/10000 (99.06%)\n",
            "\n",
            "EPOCH: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.007959898561239243 Batch_id=468 Accuracy=98.59: 100%|██████████| 469/469 [00:14<00:00, 33.38it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0297, Accuracy: 9915/10000 (99.15%)\n",
            "\n",
            "EPOCH: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.045575570315122604 Batch_id=468 Accuracy=98.95: 100%|██████████| 469/469 [00:14<00:00, 32.12it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "EPOCH: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.02830515243113041 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:13<00:00, 34.05it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0228, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "EPOCH: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.057802408933639526 Batch_id=468 Accuracy=99.05: 100%|██████████| 469/469 [00:14<00:00, 33.44it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0222, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.006628651171922684 Batch_id=468 Accuracy=99.09: 100%|██████████| 469/469 [00:13<00:00, 33.59it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0222, Accuracy: 9931/10000 (99.31%)\n",
            "\n",
            "EPOCH: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04232572019100189 Batch_id=468 Accuracy=99.11: 100%|██████████| 469/469 [00:14<00:00, 33.19it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0217, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "EPOCH: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.009203249588608742 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:14<00:00, 33.04it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0220, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.04883606731891632 Batch_id=468 Accuracy=99.13: 100%|██████████| 469/469 [00:14<00:00, 33.29it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0214, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "EPOCH: 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.024366678670048714 Batch_id=468 Accuracy=99.12: 100%|██████████| 469/469 [00:14<00:00, 32.61it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0218, Accuracy: 9938/10000 (99.38%)\n",
            "\n",
            "EPOCH: 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.031085282564163208 Batch_id=468 Accuracy=99.08: 100%|██████████| 469/469 [00:14<00:00, 33.41it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0216, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "EPOCH: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss=0.006246623117476702 Batch_id=468 Accuracy=99.17: 100%|██████████| 469/469 [00:14<00:00, 32.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0219, Accuracy: 9934/10000 (99.34%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_Zs1gaWN5t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}